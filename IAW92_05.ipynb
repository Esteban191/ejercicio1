{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_a0F6RKA7q3T"},"source":["# Dimensionality Reduction Using Feature Extraction\n","\n","Feature extraction: how to reduce the dimensionality of our feature matrix by\n","creating new features with (ideally) similar ability to train quality models but with\n","significantly fewer dimensions.\n","\n","- One downside of the feature extraction techniques is that the new features\n","we generate will not be interpretable by humans. They will contain as much or nearly\n","as much ability to train our models, but will appear to the human eye as a collection\n","of random numbers.\n","<!-- 2018, Albon, Machine Learning with Python Cookbook. Cap 9-10 -->"]},{"cell_type":"markdown","metadata":{"id":"OtJR-S5k7v_c"},"source":["## Reducing Features Using Principal Components\n","\n","Principal component analysis (PCA) projects observations onto the (hopefully fewer) principal components of the feature matrix that retain the most variance. PCA is an unsupervised technique, meaning that it does not use the information from the target vector and instead only considers the feature matrix.\n","\n","\n","![pca](https://sebastianraschka.com/images/faq/lda-vs-pca/pca.png)\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"]},{"cell_type":"code","metadata":{"id":"MlTM6Q514zBP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7cbb24a8-2071-4551-889b-63a3090cf44b","executionInfo":{"status":"ok","timestamp":1678318617164,"user_tz":300,"elapsed":520,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## Given a set of features, you want to reduce the number of features while retaining \n","## the variance in the data.\n","\n","# Load libraries\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn import datasets\n","\n","# Load the data\n","#digitos del cero al nueve a mano\n","digits = datasets.load_digits()\n","\n","# Standardize the feature matrix\n","features = StandardScaler().fit_transform(digits.data)\n","\n","# Create a PCA that will retain 99% of variance\n","pca = PCA(n_components=0.99, whiten=True)\n","\n","# Conduct PCA\n","features_pca = pca.fit_transform(features)\n","\n","# Show results\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_pca.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 64\n","Reduced number of features: 16\n"]}]},{"cell_type":"markdown","metadata":{"id":"MZ7jj47qCtPS"},"source":["PCA is implemented in scikit-learn using the `pca` method. `n_components` has two operations, depending on the argument provided. \n","- If the argument is greater than 1, `n_components` will return that many features. \n","- If the argument to n_components is between 0 and 1, `pca` returns the minimum amount of features that retain that much variance. \n","\n","It is common to use values of 0.95 and 0.99, meaning 95%\n","and 99% of the variance of the original features has been retained, respectively.\n","\n","`whiten=True` transforms the values of each principal component so that they have zero mean and unit variance. \n","\n","Another parameter and argument is\n","`svd_solver=\"randomized\"`, which implements a stochastic algorithm to find the first\n","principal components in often significantly less time."]},{"cell_type":"markdown","metadata":{"id":"l0nH9hQH8LDc"},"source":["## Reducing Features When Data Is Linearly Inseparable\n","\n","If your data is not linearly separable (e.g., you\n","can only separate classes using a curved decision boundary), the linear transformation will not work as well. In our solution we used scikit-learn’s `make_circles` to generate a simulated dataset with a target vector of two classes and two features.\n","\n","Kernels allow us to project the linearly inseparable data into a higher dimension where it is linearly separable; this is called the kernel trick. There are a number of kernels we can use in scikit-learn’s `kernelPCA`, specified using the kernel parameter. A common kernel to use is the Gaussian radial basis function kernel `rbf`, but other options are the polynomial kernel\n","(`poly`) and sigmoid kernel (`sigmoid`). We can even specify a linear projection (`linear`), which will produce the same results as standard PCA.\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA"]},{"cell_type":"code","metadata":{"id":"-vqJfN7R8NAx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d5de113-8d78-4563-c6a0-5276102e910a","executionInfo":{"status":"ok","timestamp":1678319105343,"user_tz":300,"elapsed":219,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## You suspect you have linearly inseparable data and want to reduce \n","## the dimensions.\n","\n","# Load libraries\n","from sklearn.decomposition import PCA, KernelPCA\n","from sklearn.datasets import make_circles\n","\n","# Create linearly inseparable data\n","features, _ = make_circles(n_samples=1000, random_state=1, noise=0.1, factor=0.1)\n","\n","# Apply kernal PCA with radius basis function (RBF) kernel\n","kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n","features_kpca = kpca.fit_transform(features)\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_kpca.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 2\n","Reduced number of features: 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"OIGA_k6D8UoL"},"source":["## Reducing Features by Maximizing Class Separability\n","\n","Linear discriminant analysis (LDA) is a classification that is also a popular technique for dimensionality reduction.\n","LDA works similarly to principal component analysis (PCA) in that it projects our\n","feature space onto a lower-dimensional space. However, in PCA we were only interested\n","in the component axes that maximize the variance in the data, while in LDA we\n","have the additional goal of maximizing the differences between classes.\n","\n","![lda](https://sebastianraschka.com/images/faq/lda-vs-pca/lda.png)\n","\n","\n","https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"]},{"cell_type":"code","metadata":{"id":"4hRZ75kq8Z9Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"87981dd8-d784-4f55-dec0-6ab163a2873b","executionInfo":{"status":"ok","timestamp":1678319844313,"user_tz":300,"elapsed":8,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## You want to reduce the features to be used by a classifier.\n","\n","# Load libraries\n","from sklearn import datasets\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","\n","# Load Iris flower dataset:\n","iris = datasets.load_iris()\n","features = iris.data\n","target = iris.target\n","\n","# Create and run an LDA, then use it to transform the features\n","lda = LinearDiscriminantAnalysis(n_components=1)\n","features_lda = lda.fit(features, target).transform(features)\n","\n","# Print the number of features\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_lda.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 4\n","Reduced number of features: 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"bZT9K2-Y8g9c"},"source":["We can use `explained_variance_ratio_` to view the amount of variance explained by each component."]},{"cell_type":"code","metadata":{"id":"5Y_7ruiS8dE7","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"71f54605-ef36-45b1-b053-1e4cc792a5d5"},"source":["lda.explained_variance_ratio_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.9912126])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"MeLPTBwv8lzt"},"source":["Specifically, we can run `LinearDiscriminantAnalysis` with `n_components` set to `None` to return the ratio of variance explained by every component feature, then calculate\n","how many components are required to get above some threshold of variance explained (often 0.95 or 0.99):"]},{"cell_type":"code","metadata":{"id":"Jwxa-pNr8nrw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4fa79e84-2b88-433d-b226-709679c5d2b0","executionInfo":{"status":"ok","timestamp":1678319548637,"user_tz":300,"elapsed":220,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["# Create and run LDA\n","lda = LinearDiscriminantAnalysis(n_components=None)\n","features_lda = lda.fit(features, target)\n","\n","# Create array of explained variance ratios\n","lda_var_ratios = lda.explained_variance_ratio_\n","\n","# Create function\n","def select_n_components(var_ratio, goal_var: float) -> int:\n","    # Set initial variance explained so far\n","    total_variance = 0.0\n","    # Set initial number of features\n","    n_components = 0\n","    # For the explained variance of each feature:\n","    for explained_variance in var_ratio:\n","        # Add the explained variance to the total\n","        total_variance += explained_variance\n","        # Add one to the number of components\n","        n_components += 1\n","        # If we reach our goal level of explained variance\n","        if total_variance >= goal_var:\n","            # End the loop\n","            break\n","    # Return the number of components\n","    return n_components\n","\n","# Run function\n","select_n_components(lda_var_ratios, 0.95)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"ZZsLNt6B8urJ"},"source":["## Reducing Features Using Matrix Factorization\n","\n","**Non-negative matrix factorization** (NMF) is an unsupervised technique for linear dimensionality reduction that factorizes\n","(i.e., breaks up into multiple matrices whose product approximates the original matrix) the feature matrix into matrices representing the latent relationship between\n","observations and their features. Intuitively, NMF can reduce dimensionality because in matrix multiplication, the two factors (matrices being multiplied) can have significantly\n","fewer dimensions than the product matrix.\n","\n","Formally, given a desired number of returned features, $r$, NMF factorizes our feature matrix such that:\n","\n","<font size='5'> $ \\mathbf{V} \\approx \\mathbf{WH} $ </font>\n","\n","where $V$ is our $d \\times n$ feature matrix (i.e., d features, n observations), $W$ is a $d \\times r$,\n","and $H$ is an $r \\times n$ matrix. \n","- By adjusting the value of $r$ we can set the amount of dimensionality reduction desired."]},{"cell_type":"code","metadata":{"id":"2xED9drs8rEr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a7898b0-4fc8-444d-be1c-2403b8c4805f","executionInfo":{"status":"ok","timestamp":1678320565824,"user_tz":300,"elapsed":539,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## You have a feature matrix of nonnegative values and want to reduce the dimensionality.\n","\n","# Load libraries\n","from sklearn.decomposition import NMF\n","from sklearn import datasets\n","\n","# Load the data\n","digits = datasets.load_digits()\n","\n","# Load feature matrix\n","features = digits.data\n","\n","# Create, fit, and apply NMF\n","nmf = NMF(n_components=10, random_state=1)\n","features_nmf = nmf.fit_transform(features)\n","\n","# Show results\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_nmf.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 64\n","Reduced number of features: 10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/decomposition/_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","metadata":{"id":"Qx1n3PIA85MU"},"source":["## Reducing Features on Sparse Data\n","\n","*Truncated Singular Value Decomposition* (TSVD) is similar to PCA and in fact, PCA actually often uses non-truncated Singular\n","Value Decomposition (SVD) in one of its steps. In regular SVD, given $d$ features,\n","SVD will create factor matrices that are $d \\times d$, whereas TSVD will return factors that\n","are $n \\times n$, where $n$ is previously specified by a parameter. The practical advantage of\n","TSVD is that unlike PCA, it works on **sparse feature matrices**.\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD"]},{"cell_type":"code","metadata":{"id":"y4Aomi_H81iZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e081c84-b2fb-4b44-ad17-484ad8114cc4","executionInfo":{"status":"ok","timestamp":1678320808812,"user_tz":300,"elapsed":296,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## You have a sparse feature matrix and want to reduce the dimensionality.\n","\n","# Load libraries\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import TruncatedSVD\n","from scipy.sparse import csr_matrix\n","from sklearn import datasets\n","import numpy as np\n","\n","# Load the data\n","digits = datasets.load_digits()\n","\n","# Standardize feature matrix\n","features = StandardScaler().fit_transform(digits.data)\n","\n","# Make sparse matrix\n","features_sparse = csr_matrix(features)\n","\n","# Create a TSVD\n","tsvd = TruncatedSVD(n_components=10)\n","\n","# Conduct TSVD on sparse matrix\n","features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n","\n","# Show results\n","print(\"Original number of features:\", features_sparse.shape[1])\n","print(\"Reduced number of features:\", features_sparse_tsvd.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 64\n","Reduced number of features: 10\n"]}]},{"cell_type":"markdown","metadata":{"id":"Url0Mn969Gig"},"source":["TSVD provides us with the ratio of the original feature matrix’s variance\n","explained by each component"]},{"cell_type":"code","metadata":{"id":"OX-gL1aO9CJ5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f068d0f8-70fd-4f44-8f2f-f291b84c0939","executionInfo":{"status":"ok","timestamp":1678320811509,"user_tz":300,"elapsed":975,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["# Sum of first three components' explained variance ratios\n","tsvd.explained_variance_ratio_[0:3].sum()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3003938539016248"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"aKEiUSpd9LrZ"},"source":["We can automate the process by creating a function that runs TSVD with `n_components` set to one less than the number of original features and then calculate the number of components that explain a desired amount of the original data’s variance:"]},{"cell_type":"code","metadata":{"id":"cWoyB3vK9Jcp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cbbb9c78-d4bf-4048-8384-6447ddd7be40","executionInfo":{"status":"ok","timestamp":1678320815444,"user_tz":300,"elapsed":6,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["# Create and run an TSVD with one less than number of features\n","tsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\n","features_tsvd = tsvd.fit(features)\n","\n","# List of explained variances\n","tsvd_var_ratios = tsvd.explained_variance_ratio_\n","\n","# Create a function\n","def select_n_components(var_ratio, goal_var):\n","    # Set initial variance explained so far\n","    total_variance = 0.0\n","    # Set initial number of features\n","    n_components = 0\n","    # For the explained variance of each feature:\n","    for explained_variance in var_ratio:\n","        # Add the explained variance to the total\n","        total_variance += explained_variance\n","        # Add one to the number of components\n","        n_components += 1\n","        # If we reach our goal level of explained variance\n","        if total_variance >= goal_var:\n","            # End the loop\n","            break\n","    # Return the number of components\n","    return n_components\n","\n","# Run function\n","select_n_components(tsvd_var_ratios, 0.95)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["40"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"C81b6zgg9SBz"},"source":["# Dimensionality Reduction Using Feature Selection\n","\n","Feature selection: selecting high-quality, informative features and\n","dropping less useful features. There are three types of feature selection methods: filter, wrapper, and embedded. \n","\n","- Filter methods select the best features by examining their statistical properties. \n","- Wrapper methods use trial and error to find the subset of features that produce models with the highest quality predictions. \n","- Embedded methods select the best feature subset as part or as an extension of a learning algorithm’s training process."]},{"cell_type":"markdown","metadata":{"id":"18olb_Ui9a9n"},"source":["## Thresholding Numerical Feature Variance\n","\n","**Variance thresholding** (VT) is one of the most basic approaches to feature selection. It is motivated by the idea that features with low variance are likely less interesting (and\n","useful) than features with high variance. VT first calculates the variance of each feature:\n","\n","<font size='5'> $ Var(x)=\\frac{1}{n}\\sum_{i=1}^{n}{(x_i-\\mu)^2} $ </font>\n","\n","where $x$ is the feature vector, $x_i$ is an individual feature value, and $\\mu$ is that feature’s\n","mean value. Next, it drops all features whose variance does not meet that threshold.\n","There are two things to keep in mind when employing VT:\n","- the variance is not centered; that is, it is in the squared unit of the feature itself. Therefore, the VT will not work when feature sets contain different units (e.g., one feature is in years while a different feature is in dollars). \n","- the variance threshold is selected manually, so we have to use our own judgment for a good value to select"]},{"cell_type":"code","metadata":{"id":"0ELgc8M19PUj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"50c3a4e0-a4a6-49d4-a216-7aaa55c7eaa9","executionInfo":{"status":"ok","timestamp":1678321138226,"user_tz":300,"elapsed":221,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## You have a set of numerical features and want to remove those with low variance (i.e.,likely containing little information).\n","## S/ Select a subset of features with variances above a given threshold:\n","\n","# Load libraries\n","from sklearn import datasets\n","from sklearn.feature_selection import VarianceThreshold\n","\n","# import some data to play with\n","iris = datasets.load_iris()\n","\n","# Create features and target\n","features = iris.data\n","target = iris.target\n","\n","# Create thresholder\n","thresholder = VarianceThreshold(threshold=.5)\n","\n","# Create high variance feature matrix\n","features_high_variance = thresholder.fit_transform(features)\n","\n","print(features.shape)\n","print(features_high_variance.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 4)\n","(150, 3)\n"]}]},{"cell_type":"code","metadata":{"id":"fG-qVUo59fJg","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"d849d0a1-f086-4cae-f0eb-c69f1dc5b6e4"},"source":["# View high variance feature matrix\n","features_high_variance[0:3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[5.1, 1.4, 0.2],\n","       [4.9, 1.4, 0.2],\n","       [4.7, 1.3, 0.2]])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"BrRcJhWR9kXV"},"source":["We can see the variance for each feature using\n","variances"]},{"cell_type":"code","metadata":{"id":"TfEZMB_G9iHp","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"030d4f7f-1034-4e80-caab-9a09103e1a55"},"source":["# View variances\n","thresholder.fit(features).variances_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.68112222, 0.18871289, 3.09550267, 0.57713289])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"kOzdol_y9rcV"},"source":["Finally, if the features have been standardized (to mean zero and unit variance), then for obvious reasons variance thresholding will not work correctly:"]},{"cell_type":"code","metadata":{"id":"n7QXgnZp9o5p","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"27f4bc96-c500-473a-a268-2a00863afe37"},"source":["# Load library\n","from sklearn.preprocessing import StandardScaler\n","\n","# Standardize feature matrix\n","scaler = StandardScaler()\n","features_std = scaler.fit_transform(features)\n","\n","# Caculate variance of each feature\n","selector = VarianceThreshold()\n","selector.fit(features_std).variances_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1., 1.])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"zmo2HpRQ93io"},"source":["## Thresholding Binary Feature Variance\n","\n","Just like with numerical features, one strategy for selecting highly informative categorical\n","features is to examine their **variances**. In binary features (i.e., Bernoulli random\n","variables), variance is calculated as:\n","\n","<font size='5'> $ Var x = p (1 − p) $ </font>\n","\n","\n","where $p$ is the proportion of observations of class 1. Therefore, by setting $p$, we can\n","remove features where the vast majority of observations are one class."]},{"cell_type":"code","metadata":{"id":"R2DfJm9n90JI","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"c54015b4-7ea6-43b3-ec04-bda6c387879f"},"source":["## You have a set of binary categorical features and want to remove those with low variance (i.e., likely containing little information).\n","## S/ Select a subset of features with a Bernoulli random variable variance above a given threshold:\n","\n","# Load library\n","from sklearn.feature_selection import VarianceThreshold\n","# Create feature matrix with:\n","# Feature 0: 80% class 0\n","# Feature 1: 80% class 1\n","# Feature 2: 60% class 0, 40% class 1\n","\n","features = [[0, 1, 0],\n","            [0, 1, 1],\n","            [0, 1, 0],\n","            [0, 1, 1],\n","            [1, 0, 0]]\n","\n","# Run threshold by variance\n","thresholder = VarianceThreshold(threshold=(.75 * (1 - .75)))\n","f_b = thresholder.fit_transform(features)\n","\n","print(np.asarray(features).shape)\n","print(f_b.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(5, 3)\n","(5, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IzJfhQrO-IFc"},"source":["## Handling Highly Correlated Features\n","\n","One problem we often run into in machine learning is **highly correlated features**. If\n","two features are highly correlated, then the information they contain is very similar,\n","and it is likely redundant to include both features. The solution to highly correlated\n","features is simple: remove one of them from the feature set."]},{"cell_type":"code","metadata":{"id":"5zj79SLT-B04","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c38149aa-6cb9-4d72-f9e8-3874ba018978","executionInfo":{"status":"ok","timestamp":1678321694873,"user_tz":300,"elapsed":742,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## You have a feature matrix and suspect some features are highly correlated.\n","## S/ Use a correlation matrix to check for highly correlated features. \n","## If highly correlated features exist, consider dropping one of the correlated features:\n","\n","# Load libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Create feature matrix with two highly correlated features\n","features = np.array([[1, 1, 1],\n","                     [2, 2, 0],\n","                     [3, 3, 1],\n","                     [4, 4, 0],\n","                     [5, 5, 1],\n","                     [6, 6, 0],\n","                     [7, 7, 1],\n","                     [8, 7, 0],\n","                     [9, 7, 1]])\n","\n","# Convert feature matrix into DataFrame\n","dataframe = pd.DataFrame(features)\n","\n","# Create correlation matrix\n","corr_matrix = dataframe.corr().abs()\n","\n","# Select upper triangle of correlation matrix\n","upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n","k=1).astype(np.bool))\n","\n","# Find index of feature columns with correlation greater than 0.95\n","to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n","\n","print(to_drop)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-21-52863aeaedf4>:28: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  k=1).astype(np.bool))\n"]}]},{"cell_type":"code","metadata":{"id":"LHwj64y_-Nvw","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"cfd39e41-52c5-440d-90e9-76acc45da853"},"source":["# Drop features\n","dataframe.drop(dataframe.columns[to_drop], axis=1).head(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   0  2\n","0  1  1\n","1  2  0\n","2  3  1"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"Q5kVh4Gx0np7"},"source":["In our solution, first we create a correlation matrix of all features:"]},{"cell_type":"code","metadata":{"id":"Js45zSuK-QO3","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"7647ef22-904d-47d5-9941-0c01cf70e335"},"source":["# Correlation matrix\n","dataframe.corr()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.000000</td>\n","      <td>0.976103</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.976103</td>\n","      <td>1.000000</td>\n","      <td>-0.034503</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>-0.034503</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          0         1         2\n","0  1.000000  0.976103  0.000000\n","1  0.976103  1.000000 -0.034503\n","2  0.000000 -0.034503  1.000000"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"h63FNP6g0ryG"},"source":["Second, we look at the upper triangle of the correlation matrix to identify pairs of highly correlated features:"]},{"cell_type":"code","metadata":{"id":"GLTjLI1D-S4h","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"183e4ae5-2ac3-41c7-cac1-abe74d5b2bcc"},"source":["# Upper triangle of correlation matrix\n","upper"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>NaN</td>\n","      <td>0.976103</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.034503</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    0         1         2\n","0 NaN  0.976103  0.000000\n","1 NaN       NaN  0.034503\n","2 NaN       NaN       NaN"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"Kq3ZXYmf00fU"},"source":["Third, we remove one feature from each of those pairs from the feature set."]},{"cell_type":"markdown","metadata":{"id":"_btD-Ndl-axZ"},"source":["## Removing Irrelevant Features for Classification\n","\n","**Chi-square statistics** examines the independence of two categorical vectors. That is, the statistic is the difference between the observed number of observations in each\n","class of a categorical feature and what we would expect if that feature was independent\n","(i.e., no relationship) with the target vector:\n","\n","<font size='5'> $ \\chi^2=\\sum_{i=1}^{n} \\frac{(O_i - E_i)^2}{E_i} $ </font>\n","\n","where $O_i$ is the number of observations in class $i$ and $E_i$ is the number of observations\n","in class $i$ we would expect if there is no relationship between the feature and target\n","vector."]},{"cell_type":"code","metadata":{"id":"pElvDUsd-YAZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"761ad6cc-1072-4a67-c11c-cbce73adacb2","executionInfo":{"status":"ok","timestamp":1678322150210,"user_tz":300,"elapsed":218,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["## You have a categorical target vector and want to remove uninformative features.\n","## S/ If the features are categorical, calculate a chi-square (χ2) statistic between each feature and the target vector:\n","\n","# Load libraries\n","from sklearn.datasets import load_iris\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2, f_classif\n","\n","# Load data\n","iris = load_iris()\n","features = iris.data\n","target = iris.target\n","\n","# Convert to categorical data by converting data to integers\n","features = features.astype(int)\n","\n","# Select two features with highest chi-squared statistics\n","chi2_selector = SelectKBest(chi2, k=2)\n","features_kbest = chi2_selector.fit_transform(features, target)\n","\n","# Show results\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_kbest.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 4\n","Reduced number of features: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"LaVfspVm-i1v"},"source":["If the features are quantitative, compute the ANOVA F-value between each feature and the target vector:"]},{"cell_type":"code","metadata":{"id":"rSC2GxnI-f_H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"85a6c4f6-5fcf-4abb-cbe2-e83237e5d556","executionInfo":{"status":"ok","timestamp":1678322153094,"user_tz":300,"elapsed":219,"user":{"displayName":"Santiago velez florez","userId":"13545400540356950128"}}},"source":["# Select two features with highest F-values\n","fvalue_selector = SelectKBest(f_classif, k=2)\n","features_kbest = fvalue_selector.fit_transform(features, target)\n","\n","# Show results\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_kbest.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original number of features: 4\n","Reduced number of features: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"NJSgcKVr-rot"},"source":["Instead of selecting a specific number of features, we can also use `SelectPercentile` to select the top `n` percent of features:"]},{"cell_type":"code","metadata":{"id":"qzoLGy12-oIo","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"e84b7ad4-a857-4651-84b4-19f0819f003d"},"source":["# Load library\n","from sklearn.feature_selection import SelectPercentile\n","\n","# Select top 75% of features with highest F-values\n","fvalue_selector = SelectPercentile(f_classif, percentile=75)\n","features_kbest = fvalue_selector.fit_transform(features, target)\n","\n","# Show results\n","print(\"Original number of features:\", features.shape[1])\n","print(\"Reduced number of features:\", features_kbest.shape[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original number of features: 4\n","Reduced number of features: 3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SGqmp0Hm3V9v"},"source":["By calculating the chi-squared statistic between a\n","feature and the target vector, we obtain a measurement of the independence between the two. \n","- If the target is independent of the feature variable, then it is irrelevant for our purposes because it contains no information we can use for classification. \n","- If the two features are highly dependent, they likely are very informative for training our model."]},{"cell_type":"markdown","metadata":{"id":"ixGS3Ge8-1HD"},"source":["## Recursively Eliminating Features\n","\n","The idea behind RFE is to train a model that contains some parameters (also called *weights* or *coefficients*) like linear regression or support vector machines repeatedly.\n","The first time we train the model, we include all the features. Then, we find the feature with the smallest parameter (notice that this assumes the features are either rescaled or standardized), meaning it is less important, and remove the feature from the feature set.\n","\n","We can use cross-validation (CV) to find the optimum number of features to keep during RFE. Specifically, in RFE with CV after every iteration, we use cross-validation to evaluate our\n","model. If CV shows that our model improved after we eliminated a feature, then we continue on to the next loop. However, if CV shows that our model got worse after we eliminated a feature, we put that feature back into the feature set and select those\n","features as the best.\n","\n","https://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html#sphx-glr-auto-examples-feature-selection-plot-rfe-with-cross-validation-py"]},{"cell_type":"code","metadata":{"id":"ZugbScvA-xyH","colab":{"base_uri":"https://localhost:8080/","height":144},"outputId":"2589266c-26bc-4ca8-acfd-91046ecb352c"},"source":["## You want to automatically select the best features to keep.\n","## S/ Use scikit-learn’s RFECV to conduct recursive feature elimination (RFE) using crossvalidation (CV). \n","##    That is, repeatedly train a model, each time removing a feature until model performance \n","##    (e.g., accuracy) becomes worse. The remaining features are the best:\n","\n","# Load libraries\n","import warnings\n","from sklearn.datasets import make_regression\n","from sklearn.feature_selection import RFECV\n","from sklearn import datasets, linear_model\n","\n","# Suppress an annoying but harmless warning\n","warnings.filterwarnings(action=\"ignore\", module=\"scipy\",\n","message=\"^internal gelsd\")\n","\n","# Generate features matrix, target vector, and the true coefficients\n","features, target = make_regression(n_samples = 10000,\n","                                   n_features = 100,\n","                                   n_informative = 2,\n","                                   random_state = 1)\n","\n","# Create a linear regression\n","ols = linear_model.LinearRegression()\n","\n","# Recursively eliminate features\n","rfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\")\n","rfecv.fit(features, target).transform(features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.00850799,  0.7031277 ],\n","       [-1.07500204,  2.56148527],\n","       [ 1.37940721, -1.77039484],\n","       ...,\n","       [-0.80331656, -1.60648007],\n","       [ 0.39508844, -1.34564911],\n","       [-0.55383035,  0.82880112]])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"YYeXL2Vh--8p"},"source":["Once we have conducted RFE, we can see the number of features we should keep:"]},{"cell_type":"code","metadata":{"id":"VXaZvEZp-7a_","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"bd15c3d3-6168-45fc-e69d-b39aa00c91cd"},"source":["# Number of best features\n","rfecv.n_features_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"FzYlHXoL_LqX"},"source":["We can also see which of those features we should keep:"]},{"cell_type":"code","metadata":{"id":"RJSTkcfQ_CHW","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"631621cc-f64d-4dcb-90e5-cf81ac18b972"},"source":["# Which categories are best\n","rfecv.support_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([False, False, False, False, False,  True, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False,  True, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False, False, False, False, False, False, False, False, False,\n","       False])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"WmUShtGJ_X5I"},"source":["We can even view the rankings of the features:"]},{"cell_type":"code","metadata":{"id":"9uWKU5oj_UxP","colab":{"base_uri":"https://localhost:8080/","height":126},"outputId":"0f1e36cd-29f3-4e44-e585-1968b520393f"},"source":["# Rank features best (1) to worst\n","rfecv.ranking_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([33, 39, 42, 20,  6,  1, 82, 35, 32,  3, 10, 72, 24, 44, 12, 49, 93,\n","       84, 94,  2, 25, 21, 78, 31, 43, 50, 47, 52, 81, 23, 61, 96, 80, 14,\n","       15, 58, 75, 29, 83,  1, 18, 68, 46, 19, 30,  5, 48, 60, 56, 69, 89,\n","        4, 79, 62, 11,  7, 98, 17, 71, 95, 54, 65,  9, 77, 53, 67, 16, 87,\n","       41, 85, 97, 70, 26, 76, 59, 99, 36, 34, 38, 90, 55, 64, 57, 88, 22,\n","       73, 86, 92, 27, 51, 66, 13, 74, 45, 40, 63, 37, 28,  8, 91])"]},"metadata":{"tags":[]},"execution_count":27}]}]}