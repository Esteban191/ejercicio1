{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dC5fnjSDFJ4w"},"source":["# Handling Numerical Data\n","<!-- 2018, Albon, Machine Learning with Python Cookbook. Cap 4 -->"]},{"cell_type":"markdown","metadata":{"id":"dPywlELIFSKA"},"source":["## Rescaling a Feature\n","Rescaling is a common preprocessing task in machine learning. Many of the algorithms\n","described later in this book will assume all features are on the same scale, typically\n","`0` to `1` or `–1` to `1`. There are a number of rescaling techniques, but one of the\n","simplest is called **min-max scaling**. Min-max scaling uses the minimum and maximum\n","values of a feature to rescale values to within a range. Specifically, min-max calculates:\n","\n","$x'_i = \\frac{x_i - min(x)}{max(x)-min(x)}$\n","\n","where $x$ is the feature vector, $x_i$ is an individual element of feature $x$, and $x’i$ is the\n","rescaled element."]},{"cell_type":"code","metadata":{"id":"JOo6bThFFF63","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0d70348-5322-4306-e6dd-91e3899b3de0"},"source":["# You need to rescale the values of a numerical feature to be between two values.\n","# Load libraries\n","import numpy as np\n","from sklearn import preprocessing\n","\n","# Create feature\n","feature = np.array([[-500.5], \n","                    [-100.1], \n","                    [0], \n","                    [100.1], \n","                    [900.9]])\n","\n","# Create scaler\n","minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))\n","\n","# Scale feature\n","scaled_feature = minmax_scale.fit_transform(feature)\n","\n","# Show feature\n","print(scaled_feature)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.        ]\n"," [0.28571429]\n"," [0.35714286]\n"," [0.42857143]\n"," [1.        ]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"d005bLAwFud0"},"source":["## Standardizing a Feature\n","A common alternative to min-max scaling is rescaling of features\n","to be approximately **standard normally distributed**. To achieve this, we use\n","standardization to transform the data such that it has a mean, $\\bar{x}$, of 0 and a standard\n","deviation, σ, of 1. Specifically, each element in the feature is transformed so that:\n","\n","$x'_i = \\frac{x_i - \\bar{x}}{\\sigma}$\n","\n","where $x’_i$ is our standardized form of $x_i$. The transformed feature represents the number\n","of standard deviations the original value is away from the feature’s mean value\n","(also called a z-score in statistics).\n","\n","Standardization is a common go-to scaling method for machine learning preprocessing\n","and in my experience is used more than min-max scaling. However, it depends on\n","the learning algorithm. For example, principal component analysis often works better\n","using standardization, while min-max scaling is often recommended for neural networks. As a general rule, I’d recommend\n","defaulting to standardization unless you have a specific reason to use an alternative."]},{"cell_type":"code","metadata":{"id":"DNRCQC13Fn5J","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46c4ef13-2a32-49e7-997c-b38218036d4d"},"source":["# You want to transform a feature to have a mean of 0 and a standard deviation of 1.\n","# Load libraries\n","import numpy as np\n","from sklearn import preprocessing\n","\n","# Create feature\n","x = np.array([[-1000.1], \n","              [-200.2], \n","              [500.5], \n","              [600.6], \n","              [9000.9]])\n","\n","# Create scaler\n","scaler = preprocessing.StandardScaler()\n","\n","# Transform the feature\n","standardized = scaler.fit_transform(x)\n","\n","# Show feature\n","print(standardized)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.76058269]\n"," [-0.54177196]\n"," [-0.35009716]\n"," [-0.32271504]\n"," [ 1.97516685]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"tJ6eWHgiGH15"},"source":["- We can see the effect of standardization by looking at the mean and standard deviation of our solution’s output:"]},{"cell_type":"code","metadata":{"id":"FWqEPW4dGEZe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c66798b7-dfe8-461a-fe91-22b77708debc"},"source":["# Print mean and standard deviation\n","print(\"Mean:\", round(x.mean()))\n","print(\"Standard deviation:\", x.std())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean: 1780\n","Standard deviation: 3655.6709067420165\n"]}]},{"cell_type":"code","metadata":{"id":"CIjRom42GMZN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"addb9c89-0427-4e8b-f9b3-7c8b681f133e"},"source":["# Print mean and standard deviation\n","print(\"Mean:\", round(standardized.mean()))\n","print(\"Standard deviation:\", standardized.std())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean: 0\n","Standard deviation: 1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"75zCcrhUGTlg"},"source":["- If our data has significant outliers, it can negatively impact our standardization by affecting the feature’s mean and variance. In this scenario, it is often helpful to instead rescale the feature using the median and quartile range."]},{"cell_type":"code","metadata":{"id":"y9uG09HxGRJ2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"18499553-b1f1-4e9c-8ed5-8e2791724caf"},"source":["# Create scaler\n","robust_scaler = preprocessing.RobustScaler()\n","\n","# Transform feature\n","robust_scaler.fit_transform(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-1.87387612],\n","       [-0.875     ],\n","       [ 0.        ],\n","       [ 0.125     ],\n","       [10.61488511]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"mZlSrYDLGdxI"},"source":["## Normalizing Observations\n","Many rescaling methods (e.g., min-max scaling and standardization) operate on features;\n","however, we can also rescale across individual observations. `Normalizer`\n","rescales the values on individual observations to have **unit norm** (the sum of their lengths is 1). This type of rescaling is often used when we have many equivalent features\n","(e.g., text classification when every word or n-word group is a feature).\n","Normalizer provides three norm options with Euclidean norm (often called L2)\n","being the default argument:\n","    \n","$|| x ||_2 = \\sqrt{x_1^2+x_2^2+\\cdots x_n^2}$\n","\n","where $x$ is an individual observation and $x_n$ is that observation’s value for the nth feature."]},{"cell_type":"code","metadata":{"id":"OVj6rRS9GY2r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"297f878f-1bd3-4c68-bcf9-b2f18feb2cff"},"source":["#You want to rescale the feature values of observations to have unit norm \n","# (a total length of 1).\n","# Load libraries\n","import numpy as np\n","from sklearn.preprocessing import Normalizer\n","\n","# Create feature matrix\n","features = np.array([[0.5, 0.5], \n","                     [1.1, 3.4], \n","                     [1.5, 20.2], \n","                     [1.63, 34.4], \n","                     [10.9, 3.3]])\n","\n","# Create normalizer\n","normalizer = Normalizer(norm=\"l2\")\n","\n","# Transform feature matrix\n","features_l2_norm = normalizer.transform(features)\n","\n","# Show feature matrix\n","print(features_l2_norm)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.70710678 0.70710678]\n"," [0.30782029 0.95144452]\n"," [0.07405353 0.99725427]\n"," [0.04733062 0.99887928]\n"," [0.95709822 0.28976368]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"3w7a3SAVG2dD"},"source":["- Alternatively, we can specify Manhattan norm (L1):\n","$|| x ||_1 = \\sum _{i=1}^{n} |x_i|$"]},{"cell_type":"code","metadata":{"id":"vr1YbPUEGjWM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7d1720bb-e257-4e0f-e860-a253bdd91df8"},"source":["# Transform feature matrix\n","features_l1_norm = Normalizer(norm=\"l1\").transform(features)\n","\n","# Show feature matrix\n","print(features_l1_norm)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.5        0.5       ]\n"," [0.24444444 0.75555556]\n"," [0.06912442 0.93087558]\n"," [0.04524008 0.95475992]\n"," [0.76760563 0.23239437]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"m1HcJMPgHB3c"},"source":["- Practically, notice that norm='l1' rescales an observation’s values so they sum to 1, which can sometimes be a desirable quality:"]},{"cell_type":"code","metadata":{"id":"8Swvqp3IG6Sj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b05b0af5-dbcd-4a63-8917-86e2627e55eb"},"source":["# Print sum\n","print(\"Sum of the first observation\\'s values:\", features_l1_norm[0, 0] + features_l1_norm[0, 1])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum of the first observation's values: 1.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"yH450p3lHtor"},"source":["## Generating Polynomial and Interaction Features\n","\n","Polynomial features are often created when we want to include the notion that there\n","exists a nonlinear relationship between the features and the target. For example, we\n","might suspect that the effect of age on the probability of having a major medical condition\n","is not constant over time but increases as age increases. We can encode that\n","nonconstant effect in a feature, $x$, by generating that feature’s higher-order forms ($x_2$, $x_3$, etc.)."]},{"cell_type":"code","metadata":{"id":"FlnoMqQOHI0L","colab":{"base_uri":"https://localhost:8080/"},"outputId":"02469e29-7e21-4712-b3a2-62da060035c2"},"source":["# You want to create polynominal and interaction features.\n","# Load libraries\n","import numpy as np\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","# Create feature matrix\n","features = np.array([[2, 3], \n","                     [2, 3], \n","                     [2, 3]])\n","\n","# Create PolynomialFeatures object\n","polynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n","\n","# Create polynomial features\n","polynomial_interaction.fit_transform(features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2., 3., 4., 6., 9.],\n","       [2., 3., 4., 6., 9.],\n","       [2., 3., 4., 6., 9.]])"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"3kiX8iTZIAtI"},"source":["The degree parameter determines the maximum degree of the polynomial. For example, degree=2 will create new features raised to the second power:\n","\n","$x_1, x_2, x_1^2 , x_2^2$\n","    \n","while degree=3 will create new features raised to the second and third power:\n","\n","$x_1, x_2, x_1^2 , x_2^2, x_1^3 , x_2^3$\n","\n","Furthermore, by default PolynomialFeatures includes interaction features: \n","\n","$x_1x_2$"]},{"cell_type":"code","metadata":{"id":"64oqpPCpH-kR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1396da9f-0b50-4de2-e445-454603f8a003"},"source":["# We can restrict the features created to only interaction features by setting interaction_only to True:\n","interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n","interaction.fit_transform(features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2., 3., 6.],\n","       [2., 3., 6.],\n","       [2., 3., 6.]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"6N0w4j0jIJX5"},"source":["## Transforming Features\n","\n","It is common to want to make some custom transformations to one or more features. \n","\n","- For example, we might want to create a feature that is the natural log of the values of the different feature. We can do this by creating a function and then mapping it to features using either scikit-learn’s `FunctionTransformer` or pandas’ `apply`. \n","\n","- In the solution we created a very simple function, `add_ten`, which added `10` to each input, but there is no reason we could not define a much more complex function."]},{"cell_type":"code","metadata":{"id":"BG4Y3JScIGzr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d0e271fd-550e-4f43-f760-3a58f02c5f6b"},"source":["# You want to make a custom transformation to one or more features.\n","# Load libraries\n","import numpy as np\n","from sklearn.preprocessing import FunctionTransformer\n","\n","# Create feature matrix\n","features = np.array([[2, 3],\n","                     [2, 3],\n","                     [2, 3]])\n","\n","# Define a simple function\n","def add_ten(x):\n","    return x + 10\n","\n","# Create transformer\n","ten_transformer = FunctionTransformer(add_ten)\n","\n","# Transform feature matrix\n","ten_transformer.transform(features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[12, 13],\n","       [12, 13],\n","       [12, 13]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"Ojb8sAoDIclV"},"source":["- We can create the same transformation in pandas using apply:"]},{"cell_type":"code","metadata":{"id":"e2GLfboOIXfB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c073bbf6-0b17-4397-ab72-1c353da0319f"},"source":["# Load library\n","import pandas as pd\n","\n","# Create DataFrame\n","df = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n","\n","# Apply function\n","df.apply(add_ten)\n","\n","df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 2)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"BVDOjSX7Intb"},"source":["## Detecting Outliers\n","\n","There is no single best technique for detecting outliers. Instead, we have a collection\n","of techniques all with their own advantages and disadvantages. Our best strategy is\n","often trying multiple techniques (e.g., both EllipticEnvelope and IQR-based detection)\n","and looking at the results as a whole."]},{"cell_type":"code","metadata":{"id":"rn2VYHYgIg7C","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4faef3e-de9d-4a61-be13-f58a042c59a4"},"source":["# You want to identify extreme observations.\n","# Load libraries\n","import numpy as np\n","from sklearn.covariance import EllipticEnvelope\n","from sklearn.datasets import make_blobs\n","\n","# Create simulated data\n","features, _ = make_blobs(n_samples = 10, \n","                         n_features = 2, \n","                         centers = 1, \n","                         random_state = 1)\n","print(features)\n","\n","# Replace the first observation's values with extreme values\n","features[0,0] = 10000\n","features[0,1] = 10000\n","print(features)\n","\n","# Create detector\n","outlier_detector = EllipticEnvelope(contamination=.1)\n","\n","# Fit detector\n","outlier_detector.fit(features)\n","\n","# Predict outliers\n","outlier_detector.predict(features)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-1.83198811  3.52863145]\n"," [-2.76017908  5.55121358]\n"," [-1.61734616  4.98930508]\n"," [-0.52579046  3.3065986 ]\n"," [ 0.08525186  3.64528297]\n"," [-0.79415228  2.10495117]\n"," [-1.34052081  4.15711949]\n"," [-1.98197711  4.02243551]\n"," [-2.18773166  3.33352125]\n"," [-0.19745197  2.34634916]]\n","[[ 1.00000000e+04  1.00000000e+04]\n"," [-2.76017908e+00  5.55121358e+00]\n"," [-1.61734616e+00  4.98930508e+00]\n"," [-5.25790464e-01  3.30659860e+00]\n"," [ 8.52518583e-02  3.64528297e+00]\n"," [-7.94152277e-01  2.10495117e+00]\n"," [-1.34052081e+00  4.15711949e+00]\n"," [-1.98197711e+00  4.02243551e+00]\n"," [-2.18773166e+00  3.33352125e+00]\n"," [-1.97451969e-01  2.34634916e+00]]\n"]},{"output_type":"execute_result","data":{"text/plain":["array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"xsaCAL7bI3T2"},"source":["A major limitation of this approach is the need to specify a `contamination` parameter,\n","which is the proportion of observations that are outliers—a value that we don’t\n","know. Think of `contamination` as our estimate of the cleanliness of our data. If we\n","expect our data to have few outliers, we can set `contamination` to something small.\n","However, if we believe that the data is very likely to have outliers, we can set it to a\n","higher value."]},{"cell_type":"markdown","metadata":{"id":"hbS-nZSyJGTI"},"source":["- Instead of looking at observations as a whole, we can instead look at individual features and identify extreme values in those features using interquartile range (IQR):"]},{"cell_type":"code","metadata":{"id":"1UhAgNSRIsCi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f12a8e9f-e36d-45d4-e7e5-812ccc7ac247"},"source":["# Create one feature\n","feature = features[:,0]\n","\n","# Create a function to return index of outliers\n","def indicies_of_outliers(x):\n","    q1, q3 = np.percentile(x, [25, 75])\n","    iqr = q3 - q1\n","    lower_bound = q1 - (iqr * 1.5)\n","    upper_bound = q3 + (iqr * 1.5)\n","    return np.where((x > upper_bound) | (x < lower_bound))\n","\n","# Run function\n","indicies_of_outliers(feature)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([0]),)"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"5jo5LNMPJVw7"},"source":["IQR is the difference between the first and third quartile of a set of data. You can\n","think of IQR as the spread of the bulk of the data, with outliers being observations far\n","from the main concentration of data. Outliers are commonly defined as any value 1.5\n","IQRs less than the first quartile or 1.5 IQRs greater than the third quartile."]},{"cell_type":"markdown","metadata":{"id":"DVDK141BJcI6"},"source":["## Handling Outliers\n","\n","Similar to detecting outliers, there is no hard-and-fast rule for handling them. How\n","we handle them should be based on two aspects.\n","\n","- First, we should consider what makes them an outlier. If we believe they are errors in the data such as from a broken sensor or a miscoded value, then we might drop the observation or replace outlier values with NaN since we can’t believe those values. However, if we believe the outliers are genuine extreme values (e.g., a house [mansion] with 200 bathrooms), then marking them as outliers or transforming their values is more appropriate.\n","\n","- Second, how we handle outliers should be based on our goal for machine learning. For example, if we want to predict house prices based on features of the house, we might reasonably assume the price for mansions with over 100 bathrooms is driven by a different dynamic than regular family homes. Furthermore, if we are training a model to use as part of an online home loan web application, we might assume that our potential users will not include billionaires looking to buy a mansion.\n","\n","So what should we do if we have outliers? Think about why they are outliers, have an\n","end goal in mind for the data, and, most importantly, remember that not making a\n","decision to address outliers is itself a decision with implications.\n","\n","One additional point: ***if you do have outliers standardization might not be appropriate***\n","because the mean and variance might be highly influenced by the outliers. In this\n","case, use a rescaling method more robust against outliers like `RobustScaler`."]},{"cell_type":"code","metadata":{"id":"qw_iol0iJJ9x","colab":{"base_uri":"https://localhost:8080/","height":143},"outputId":"e58f9512-0001-4a21-a30d-66182ba01173"},"source":["# You have outliers. 1. we can drop them:\n","#\n","# Load library\n","import pandas as pd\n","\n","# Create DataFrame\n","houses = pd.DataFrame()\n","houses['Price'] = [534433, 392333, 293222, 4322032]\n","houses['Bathrooms'] = [2, 3.5, 2, 116]\n","houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n","\n","# Filter observations\n","houses[houses['Bathrooms'] < 20]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-9611157c-3a1a-44e0-912c-4c01eda5d5ed\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Price</th>\n","      <th>Bathrooms</th>\n","      <th>Square_Feet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>534433</td>\n","      <td>2.0</td>\n","      <td>1500</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>392333</td>\n","      <td>3.5</td>\n","      <td>2500</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>293222</td>\n","      <td>2.0</td>\n","      <td>1500</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9611157c-3a1a-44e0-912c-4c01eda5d5ed')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9611157c-3a1a-44e0-912c-4c01eda5d5ed button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9611157c-3a1a-44e0-912c-4c01eda5d5ed');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    Price  Bathrooms  Square_Feet\n","0  534433        2.0         1500\n","1  392333        3.5         2500\n","2  293222        2.0         1500"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"h5Slm0Q9JnRA","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"025e80c4-b420-4292-8863-c1407833144b"},"source":["# You have outliers. 2. we can mark them as outliers and include it as a feature\n","\n","# Load library\n","import numpy as np\n","\n","# Create feature based on boolean condition\n","houses[\"Outlier\"] = np.where(houses[\"Bathrooms\"] < 20, 0, 1)\n","\n","# Show data\n","houses"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-fbeda502-e8ae-41aa-8996-caaef2b57e7d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Price</th>\n","      <th>Bathrooms</th>\n","      <th>Square_Feet</th>\n","      <th>Outlier</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>534433</td>\n","      <td>2.0</td>\n","      <td>1500</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>392333</td>\n","      <td>3.5</td>\n","      <td>2500</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>293222</td>\n","      <td>2.0</td>\n","      <td>1500</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4322032</td>\n","      <td>116.0</td>\n","      <td>48000</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbeda502-e8ae-41aa-8996-caaef2b57e7d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fbeda502-e8ae-41aa-8996-caaef2b57e7d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fbeda502-e8ae-41aa-8996-caaef2b57e7d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     Price  Bathrooms  Square_Feet  Outlier\n","0   534433        2.0         1500        0\n","1   392333        3.5         2500        0\n","2   293222        2.0         1500        0\n","3  4322032      116.0        48000        1"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"9XpG4JaIKUmv","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"4f4092c6-f78a-4af0-c220-322bc35679fd"},"source":["# You have outliers. 3. we can transform the feature to dampen the effect of the outlier:\n","\n","# Log feature\n","houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]]\n","\n","# Show data\n","houses"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-59360d18-31db-4738-ac78-97617540471e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Price</th>\n","      <th>Bathrooms</th>\n","      <th>Square_Feet</th>\n","      <th>Outlier</th>\n","      <th>Log_Of_Square_Feet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>534433</td>\n","      <td>2.0</td>\n","      <td>1500</td>\n","      <td>0</td>\n","      <td>7.313220</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>392333</td>\n","      <td>3.5</td>\n","      <td>2500</td>\n","      <td>0</td>\n","      <td>7.824046</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>293222</td>\n","      <td>2.0</td>\n","      <td>1500</td>\n","      <td>0</td>\n","      <td>7.313220</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4322032</td>\n","      <td>116.0</td>\n","      <td>48000</td>\n","      <td>1</td>\n","      <td>10.778956</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59360d18-31db-4738-ac78-97617540471e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-59360d18-31db-4738-ac78-97617540471e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-59360d18-31db-4738-ac78-97617540471e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     Price  Bathrooms  Square_Feet  Outlier  Log_Of_Square_Feet\n","0   534433        2.0         1500        0            7.313220\n","1   392333        3.5         2500        0            7.824046\n","2   293222        2.0         1500        0            7.313220\n","3  4322032      116.0        48000        1           10.778956"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"G7975J9TKoff"},"source":["## Discretizating Features\n","\n","Discretization can be a fruitful strategy when we have reason to believe that a numerical\n","feature should behave more like a categorical feature. For example, we might\n","believe there is very little difference in the spending habits of 19- and 20-year-olds,\n","but a significant difference between 20- and 21-year-olds (the age in the United States\n","when young adults can consume alcohol). In that example, it could be useful to break\n","up individuals in our data into those who can drink alcohol and those who cannot.\n","Similarly, in other cases it might be useful to discretize our data into three or more\n","bins.\n","\n","1. First, we can binarize the feature according to some threshold:"]},{"cell_type":"code","metadata":{"id":"0q0UFfG9KZBI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aae2788b-c8a1-40a0-9ffe-6b8131d73889"},"source":["# You have a numerical feature and want to break it up into discrete bins.\n","\n","# Load libraries\n","import numpy as np\n","from sklearn.preprocessing import Binarizer\n","\n","# Create feature\n","age = np.array([[6], \n","                [12], \n","                [20], \n","                [36], \n","                [65]])\n","\n","# Create binarizer\n","binarizer = Binarizer(threshold=18)\n","\n","# Transform feature\n","binarizer.fit_transform(age)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0],\n","       [0],\n","       [1],\n","       [1],\n","       [1]])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"vMG5kFuiKsv_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a14d272-d1a0-40db-d37f-99e38fb04313"},"source":["# Bin feature\n","np.digitize(age, bins=[18])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0],\n","       [0],\n","       [1],\n","       [1],\n","       [1]])"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"96ZuJHMNK26j"},"source":["2. we can break up numerical features according to multiple thresholds:"]},{"cell_type":"code","metadata":{"id":"GpFEfbaqK1AP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a00d48f7-6de5-4e5b-ba60-2600fa759373"},"source":["# Bin feature\n","print(age)\n","print(np.digitize(age, bins=[20,30,64]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 6]\n"," [12]\n"," [20]\n"," [36]\n"," [65]]\n","[[0]\n"," [0]\n"," [1]\n"," [2]\n"," [3]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"DwsUDN4VLADh"},"source":["## Grouping Observations Using Clustering\n","I wanted to point out that we can use clustering as a preprocessing step. Specifically, we use unsupervised learning algorithms\n","like k-means to cluster observations into groups. The end result is a categorical feature with similar observations being members of the same group."]},{"cell_type":"code","metadata":{"id":"imdyJjzRK6hH","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"f0c1cbfd-959d-44f9-c2ff-000909594bc1"},"source":["# You want to cluster observations so that similar observations are grouped together.\n","\n","# Load libraries\n","import pandas as pd\n","from sklearn.datasets import make_blobs\n","from sklearn.cluster import KMeans\n","\n","# Make simulated feature matrix\n","features, _ = make_blobs(n_samples = 50, \n","                         n_features = 2, \n","                         centers = 3, \n","                         random_state = 1)\n","\n","# Create DataFrame\n","dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n","\n","# Make k-means clusterer\n","clusterer = KMeans(3, random_state=0)\n","\n","# Fit clusterer\n","clusterer.fit(features)\n","\n","# Predict values\n","dataframe[\"group\"] = clusterer.predict(features)\n","\n","# View first few observations\n","dataframe.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-31a8f669-6019-4b07-8396-5cd7a93d8277\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","      <th>group</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-9.877554</td>\n","      <td>-3.336145</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-7.287210</td>\n","      <td>-8.353986</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-6.943061</td>\n","      <td>-7.023744</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-7.440167</td>\n","      <td>-8.791959</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-6.641388</td>\n","      <td>-8.075888</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>-0.794152</td>\n","      <td>2.104951</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>-2.760179</td>\n","      <td>5.551214</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>-9.946905</td>\n","      <td>-4.590344</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>-0.525790</td>\n","      <td>3.306599</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>-1.981977</td>\n","      <td>4.022436</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31a8f669-6019-4b07-8396-5cd7a93d8277')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-31a8f669-6019-4b07-8396-5cd7a93d8277 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-31a8f669-6019-4b07-8396-5cd7a93d8277');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   feature_1  feature_2  group\n","0  -9.877554  -3.336145      0\n","1  -7.287210  -8.353986      2\n","2  -6.943061  -7.023744      2\n","3  -7.440167  -8.791959      2\n","4  -6.641388  -8.075888      2\n","5  -0.794152   2.104951      1\n","6  -2.760179   5.551214      1\n","7  -9.946905  -4.590344      0\n","8  -0.525790   3.306599      1\n","9  -1.981977   4.022436      1"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"2pQrkXYgLF1u"},"source":["## Deleting Observations with Missing Values\n","\n","Most machine learning algorithms cannot handle any missing values in the target and\n","feature arrays. For this reason, we cannot ignore missing values in our data and must\n","address the issue during preprocessing.\n","\n","The simplest solution is to delete every observation that contains one or more missing\n","values, a task quickly and easily accomplished using NumPy or pandas."]},{"cell_type":"code","metadata":{"id":"MI7V5GW6LC8X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b796c56d-bda2-4574-c4c6-48e0aa24c2f9"},"source":["# You need to delete observations containing missing values.\n","# Load library\n","import numpy as np\n","\n","# Create feature matrix\n","features = np.array([[1.1, 11.1], \n","                     [2.2, 22.2],\n","                     [3.3, 33.3],\n","                     [4.4, 44.4],\n","                     [np.nan, 55]])\n","\n","# Keep only observations that are not (denoted by ~) missing\n","features[~np.isnan(features).any(axis=1)]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.1, 11.1],\n","       [ 2.2, 22.2],\n","       [ 3.3, 33.3],\n","       [ 4.4, 44.4]])"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"cNbUjhwuLLZ1"},"source":["- Alternatively, we can drop missing observations using pandas:"]},{"cell_type":"code","metadata":{"id":"6WhDpYl9LIUd","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"eafdb649-fd84-4837-d8a8-ff7594cb5a53"},"source":["# Load library\n","import pandas as pd\n","\n","# Load data\n","dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n","\n","# Remove observations with missing values\n","dataframe.dropna()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-1c4b524e-3a24-42e4-bdf0-eaf4722431bf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>feature_1</th>\n","      <th>feature_2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.1</td>\n","      <td>11.1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2.2</td>\n","      <td>22.2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3.3</td>\n","      <td>33.3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.4</td>\n","      <td>44.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c4b524e-3a24-42e4-bdf0-eaf4722431bf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1c4b524e-3a24-42e4-bdf0-eaf4722431bf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1c4b524e-3a24-42e4-bdf0-eaf4722431bf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   feature_1  feature_2\n","0        1.1       11.1\n","1        2.2       22.2\n","2        3.3       33.3\n","3        4.4       44.4"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"44e_hIcALWwD"},"source":["## Imputing Missing Values\n","\n","There are two main strategies for replacing missing data with substitute values, each\n","of which has strengths and weaknesses.\n","\n","- First, we can use machine learning to predict the values of the missing data. To do this we treat the feature with missing values as a target vector and use the remaining subset of features to predict missing values.\n","\n","- An alternative and more scalable strategy is to fill in all missing values with some average value."]},{"cell_type":"code","source":["!pip install fancyimpute"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bNcENhAONnC","outputId":"2302b7eb-2198-4a96-fd36-6963fd168bf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fancyimpute\n","  Downloading fancyimpute-0.7.0.tar.gz (25 kB)\n","Collecting knnimpute>=0.1.0\n","  Downloading knnimpute-0.1.0.tar.gz (8.3 kB)\n","Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.0.2)\n","Requirement already satisfied: cvxpy in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.0.31)\n","Requirement already satisfied: cvxopt in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (1.2.7)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from fancyimpute) (3.6.4)\n","Collecting nose\n","  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n","\u001b[K     |████████████████████████████████| 154 kB 6.9 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.15.0)\n","Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from knnimpute>=0.1.0->fancyimpute) (1.21.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->fancyimpute) (1.4.1)\n","Requirement already satisfied: scs>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (3.2.0)\n","Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (0.6.2.post0)\n","Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (2.0.10)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from cvxpy->fancyimpute) (0.70.12.2)\n","Requirement already satisfied: qdldl in /usr/local/lib/python3.7/dist-packages (from osqp>=0.4.1->cvxpy->fancyimpute) (0.1.5.post0)\n","Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.7/dist-packages (from multiprocess->cvxpy->fancyimpute) (0.3.4)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (1.11.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (1.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (21.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (57.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (0.7.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->fancyimpute) (8.12.0)\n","Building wheels for collected packages: fancyimpute, knnimpute\n","  Building wheel for fancyimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fancyimpute: filename=fancyimpute-0.7.0-py3-none-any.whl size=29899 sha256=44f03542e435f85433c0722dd2185781507846128ee1d4392e7e556ce3e96afc\n","  Stored in directory: /root/.cache/pip/wheels/e3/04/06/a1a7d89ef4e631ce6268ea2d8cde04f7290651c1ff1025ce68\n","  Building wheel for knnimpute (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for knnimpute: filename=knnimpute-0.1.0-py3-none-any.whl size=11353 sha256=ecffbe9c2120eb415db979c76c9402922076a2c18b4ff7814562be9f283c0d01\n","  Stored in directory: /root/.cache/pip/wheels/72/21/a8/a045cacd9838abd5643f6bfa852c0796a99d6b1494760494e0\n","Successfully built fancyimpute knnimpute\n","Installing collected packages: nose, knnimpute, fancyimpute\n","Successfully installed fancyimpute-0.7.0 knnimpute-0.1.0 nose-1.3.7\n"]}]},{"cell_type":"code","metadata":{"id":"FJ7EOol-LSYf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2a110cdb-2fa2-408c-b7fd-d89bdd2a1076"},"source":["# You have missing values in your data and want to fill in or predict their values.\n","\n","# Load libraries\n","import numpy as np\n","from fancyimpute import KNN\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_blobs\n","\n","# Make a simulated feature matrix\n","features, _ = make_blobs(n_samples = 1000,\n","                        n_features = 2,\n","                        random_state = 1)\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","standardized_features = scaler.fit_transform(features)\n","\n","# Replace the first feature's first value with a missing value\n","true_value = standardized_features[0,0]\n","standardized_features[0,0] = np.nan\n","\n","# Predict the missing values in the feature matrix\n","features_knn_imputed = KNN(k=5, verbose=0).fit_transform(standardized_features)\n","\n","# Compare true and imputed values\n","print(\"True Value:\", true_value)\n","print(\"Imputed Value:\", features_knn_imputed[0,0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True Value: 0.8730186113995938\n","Imputed Value: 1.0955332713113226\n"]}]},{"cell_type":"markdown","metadata":{"id":"IlAQ_XpKL2Kx"},"source":["- Alternatively, we can use scikit-learn’s Imputer module to fill in missing values with the feature’s mean, median, or most frequent value."]},{"cell_type":"code","metadata":{"id":"Xxd0TOQcLk11","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb855260-1c1f-4d74-ac54-da6378c4813f"},"source":["# Load library\n","from sklearn.impute import SimpleImputer\n","\n","# Create imputer\n","mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","\n","# Impute values\n","features_mean_imputed = mean_imputer.fit(features)\n","features_mean_imputed = mean_imputer.transform(features)\n","\n","# Compare true and imputed values\n","print(\"True Value:\", true_value)\n","print(\"Imputed Value:\", features_mean_imputed[0,0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True Value: 0.8730186113995938\n","Imputed Value: -3.058372724614996\n"]}]},{"cell_type":"code","metadata":{"id":"Ym0M049WL7m1"},"source":[],"execution_count":null,"outputs":[]}]}